# --- EvoQ specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 100000 # 500000 for 6h_vs_8z

runner: "parallel_robust"
batch_size_run: 8 # batch_size_run=4, buffer_size = 2500, batch_size=64  for 3s5z_vs_3s6z
buffer_size: 5000 
batch_size: 128
optimizer: 'adam'

t_max: 10050000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
mac: "attack_mac"
agent: "n_rnn"
agent_output_type: q

learner: "EvoQ_learner"
mixer: "qmix"
mixing_embed_dim: 32
hypernet_embed: 64
lr: 0.001 # Learning rate for agents
td_lambda: 0.6 # 0.3 for 6h_vs_8z
optimizer: 'adam'
q_lambda: False

# rnn layer normalization
use_layer_norm: False

# orthogonal init for DNN
use_orthogonal: False
gain: 0.01

# Priority experience replay
use_per: False
per_alpha: 0.6
per_beta: 0.4
return_priority: False


# --- Evolution settings ---
pop_size: 10
elite_size: 0.2

# --- Evolutionary Mechanisms ---

# Mechanism 1: Twin Adversarial Confidence (Robustness Metric)
use_twin_adversarial_confidence: True  # Use Twin-Q networks for robust confidence estimation
adversarial_tau: 1.0                    # Boltzmann temperature for policy distribution (softmax temperature)

# Mechanism 2: Evolutionary Consensus (Population-Level Robustness Calibration)
use_evolutionary_consensus: True       # Enable population consensus metric (swarm intelligence)
consensus_elite_size: 3                 # Number of elite agents for consensus calculation
consensus_fault_type: "worst"           # Fault injection type for consensus: "worst", "random", "zero"

# Mechanism 3: Adversarial Behavioral Novelty (Quality-Diversity)
use_adversarial_novelty: True          # Enable archive-based novelty search (prevent convergence)
archive_max_size: 50                    # Maximum size of elite behavior archive
novelty_k_nearest: 5                    # K for K-nearest neighbor novelty calculation
novelty_delta: 0.1                      # Perturbation magnitude for influence calculation

# Mechanism 4: Byzantine Fault Tolerance - Fault Isolation Ratio
fault_type: "worst"                     # Byzantine fault type: "worst", "random", "zero"

# Mechanism 5: Byzantine Fault Tolerance - Influence Constraint (Power Balance)
influence_delta: 0.1                    # δ for measuring agent influence: Q_k → Q_k + δ
influence_lambda: 0.5                   # λ weight for influence variance penalty

# Learning-Assisted Dynamic Weighting (Adaptive Phase-specific Multi-Objective Optimization)
use_linear_alpha: True                 # Use linear ramp (True) or sigmoid (False) for alpha_t
alpha_steepness: 10.0                   # Steepness (k) of sigmoid for alpha_t transfer function
alpha_center: 0.5                       # Center point (P_center) for sigmoid
alpha_start: 0.3                        # Start point (P_start) for linear ramp
alpha_end: 0.8                          # End point (P_end) for linear ramp
td_init_steps: 50                       # Number of training steps to compute baseline TD_init

# Lamarckian Co-evolution (SGD Injection into Evolution)
use_lamarckian_sgd: True                # Enable Lamarckian SGD finetuning of elites
num_finetune_elites: 3                  # Number of top elites to finetune with SGD
lamarckian_sgd_steps: 1                 # Number of SGD steps per elite
finetune_lr: 0.0001                     # Learning rate for Lamarckian finetuning (conservative)
max_finetune_loss: 100.0                # Maximum loss threshold for finetuning (prevent divergence)
finetune_grad_clip: 5.0                 # Gradient clipping for finetuning (prevent explosion)

# RL → Evolution Injection Strategy
use_rl_injection: True                 # Enable legacy RL → Evo injection (replaced by new logic)


# --- Attacker parameters ---
population_train_steps: 6
population_train_num: 1
individual_sample_episode: 4
save_archive_interval: 400
select_strategy: "random"
spare_lambda: 0.04
attack_batch_size: 10
min_jsdloss_sample: 64
attacker_eval_num: 4
smoothing_factor: 0.02
gen_random_start: 0.4
gen_random_end: 0.05
threshold_ratio_start: 0.9
threshold_ratio_end: 0.5
attack_nepisode: 5
default_nepisode: 5
truncation: False
shaping_reward: True
one_buffer: True
#seed: 2
concat_left_time: True
penalty: True
penalty_weight: 0.5
long_eval_interval: 100
attack_lr: 0.0002
ego_train_step: 10
go_on: False
train_attack_threshold: 0

archive_size: 15
jsd_beta: 1.2
eval_num: 1000
generation: 800
finetune_gen: 500
archive_load_path: ""
ego_agent_path: ""
start_eval: False
gpu_id: 0
attack_num: 6
test_attack_num: 6
fine_tune: False
pop_size: 4
run_type: "robust" # robust, attack, attack_na, robust_na, eval_na
test_attacker_archive_path: ""
diversity: True
sparse_ref_delta: 0.05
#sparse_ref_delta: 0
load_sparse_ref_delta: 0.05
train_random: False
attacker_hidden_dim: 64
attacker_soft_tau: 0.005
attack_agent_selector: "sparse"
adversarial_training: True
num_attack_train: 12
attack_mode: "action"


# ========================================================================
# Multi-Objective Evolutionary Adversarial Training (MOEA-AAG)
# ========================================================================
# This section configures the adversarial training system using 
# Multi-Objective Evolutionary Algorithms (MOEA) with NSGA-II.
#
# Key Features:
# 1. Three-objective optimization: Damage, Cost, Diversity
# 2. Pareto-based selection (no hyperparameter tuning for α, β)
# 3. Implicit curriculum learning (automatic difficulty adjustment)
# 4. Behavioral diversity (corner case coverage)
#
# Advantages over ROMANCE/WALL:
# - No manual hyperparameter tuning (α, β weights)
# - Automatic difficulty curriculum (based on Pareto front evolution)
# - Comprehensive attack coverage (entire Pareto front, not single point)
# - Mode collapse prevention (diversity objective + behavioral archive)
# ========================================================================

# --- Core Adversarial Training Switch ---
use_adversarial_training: False         # Enable MOEA-based adversarial training
                                        # Set to True to activate co-evolution

# --- Attacker Population Settings ---
attacker_pop_size: 20                   # Attacker population size (N attackers)
                                        # Larger = more diverse attacks, but more computation
                                        # Recommended: 15-30 for SMAC environments

attacker_elite_fraction: 0.2            # Fraction of population in Pareto front to preserve
                                        # 0.2 means top 20% (4 attackers if pop=20)
                                        # This ensures best attackers survive evolution

# --- NSGA-II Evolutionary Parameters ---
attacker_mutation_prob: 0.9             # Probability of mutation per offspring
                                        # High value (0.8-0.95) recommended for exploration

attacker_crossover_prob: 0.9            # Probability of crossover when generating offspring
                                        # High value ensures parameter mixing

attacker_tournament_size: 3             # Tournament size for selection
                                        # 3 is standard for NSGA-II

# --- Lamarckian Gradient Mutation (Key Innovation!) ---
attacker_use_lamarckian: True           # Enable gradient-based mutation
                                        # This is the key feature combining EA + RL
                                        # Attackers learn to attack current defender via SGD

attacker_lamarckian_lr: 0.001           # Learning rate for Lamarckian mutation
                                        # Conservative value to avoid overfitting

attacker_lamarckian_steps: 5            # Number of gradient steps per mutation
                                        # More steps = stronger adaptation to defender
                                        # 3-10 recommended

# --- Behavioral Diversity Archive (Quality-Diversity) ---
attacker_archive_size: 50               # Maximum size of behavioral archive
                                        # Stores behavioral descriptors for novelty search
                                        # Larger = better diversity tracking, more memory

# --- Attack Budget and Sparsity ---
epsilon: 0.2                            # L_inf perturbation budget (attack strength)
                                        # 0.1-0.3 typical for observation perturbations
                                        # Higher = stronger attacks, less stealthy

attacker_sparsity_target: 0.3           # Target attack frequency (1 - sparsity)
                                        # 0.3 means attack 30% of timesteps
                                        # Lower = more stealthy, higher = more aggressive
                                        # This is f2 (cost) objective target

# --- Attacker Evaluation ---
attacker_eval_episodes: 5               # Episodes to evaluate each attacker's fitness
                                        # More = more accurate fitness, slower evolution
                                        # 3-10 recommended

# --- Training Schedule and Curriculum ---
adversarial_warmup_steps: 50000         # Timesteps before attackers are introduced
                                        # Defender learns basic policy first
                                        # Recommended: 10-20% of t_max

attacker_evolution_freq: 100            # Evolve attacker population every N episodes
                                        # Lower = attackers adapt faster, more computation
                                        # 50-200 recommended

defender_training_freq: 1               # Train defender every N episodes
                                        # Usually 1 (train every episode)

use_implicit_curriculum: True           # Enable automatic difficulty adjustment
                                        # Attack probability increases with training progress
                                        # Key feature for implicit curriculum learning

max_attack_probability: 0.5             # Maximum probability of using attacks during training
                                        # 0.5 means 50% clean episodes, 50% adversarial
                                        # Balance between clean and robust training

# --- Attack Diversity Sampling ---
# During defender training, we sample diverse attackers from Pareto front:
# - Best damage attacker (strongest)
# - Best sparsity attacker (stealthiest)  
# - Best diversity attacker (most novel)
# This ensures defender sees different attack types, not just strongest

# --- Observation Space Constraints (optional) ---
# obs_clip_min: -10.0                   # Minimum valid observation value
# obs_clip_max: 10.0                    # Maximum valid observation value
                                        # Uncomment if environment has observation bounds

# --- Computational Budget ---
# Total computation per attacker evolution:
#   attacker_pop_size × attacker_eval_episodes × episode_length
# Example: 20 × 5 × 100 = 10,000 timesteps per evolution
# If attacker_evolution_freq = 100 episodes, this is 10% overhead
# Adjust attacker_pop_size and attacker_eval_episodes to control computation

# --- Recommended Configurations ---
#
# Configuration 1: Basic (Fast, Low Computation)
# - attacker_pop_size: 10
# - attacker_eval_episodes: 3
# - attacker_evolution_freq: 200
# - Use for quick experiments and debugging
#
# Configuration 2: Standard (Balanced)
# - attacker_pop_size: 20
# - attacker_eval_episodes: 5
# - attacker_evolution_freq: 100
# - Use for most SMAC environments
#
# Configuration 3: Complete (High Quality, High Computation)
# - attacker_pop_size: 30
# - attacker_eval_episodes: 10
# - attacker_evolution_freq: 50
# - Use for final experiments and paper results

name: "EvoQ_env=8_adam_td_lambda"
