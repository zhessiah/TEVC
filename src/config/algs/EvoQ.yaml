# --- EvoQ specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 100000 # 500000 for 6h_vs_8z

runner: "parallel_robust"
batch_size_run: 8 # batch_size_run=4, buffer_size = 2500, batch_size=64  for 3s5z_vs_3s6z
buffer_size: 5000 
batch_size: 128
optimizer: 'adam'

t_max: 10050000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
mac: "attack_mac"
agent: "n_rnn"
agent_output_type: q

learner: "EvoQ_learner"
mixer: "qmix"
mixing_embed_dim: 32
hypernet_embed: 64
lr: 0.001 # Learning rate for agents
td_lambda: 0.6 # 0.3 for 6h_vs_8z
q_lambda: False

# rnn layer normalization
use_layer_norm: False

# orthogonal init for DNN
use_orthogonal: False
gain: 0.01

# Priority experience replay
use_per: False
per_alpha: 0.6
per_beta: 0.4
return_priority: False


# --- Evolution settings ---
pop_size: 10
elite_size: 0.2

# --- Evolutionary Mechanisms ---

# Mechanism 1: Twin Adversarial Confidence (Robustness Metric)
use_twin_adversarial_confidence: True  # Use Twin-Q networks for robust confidence estimation
adversarial_tau: 1.0                    # Boltzmann temperature for policy distribution (softmax temperature)

# Mechanism 2: Evolutionary Consensus (Population-Level Robustness Calibration)
use_evolutionary_consensus: True       # Enable population consensus metric (swarm intelligence)
consensus_elite_size: 3                 # Number of elite agents for consensus calculation
consensus_fault_type: "worst"           # Fault injection type for consensus: "worst", "random", "zero"

# Mechanism 3: Adversarial Behavioral Novelty (Quality-Diversity)
use_adversarial_novelty: True          # Enable archive-based novelty search (prevent convergence)
archive_max_size: 50                    # Maximum size of elite behavior archive
novelty_k_nearest: 5                    # K for K-nearest neighbor novelty calculation
novelty_delta: 0.1                      # Perturbation magnitude for influence calculation

# Mechanism 4: Byzantine Fault Tolerance - Fault Isolation Ratio
fault_type: "worst"                     # Byzantine fault type: "worst", "random", "zero"

# Mechanism 5: Byzantine Fault Tolerance - Influence Constraint (Power Balance)
influence_delta: 0.1                    # δ for measuring agent influence: Q_k → Q_k + δ
influence_lambda: 0.5                   # λ weight for influence variance penalty

# Learning-Assisted Dynamic Weighting (Adaptive Phase-specific Multi-Objective Optimization)
use_linear_alpha: True                 # Use linear ramp (True) or sigmoid (False) for alpha_t
alpha_steepness: 10.0                   # Steepness (k) of sigmoid for alpha_t transfer function
alpha_center: 0.5                       # Center point (P_center) for sigmoid
alpha_start: 0.3                        # Start point (P_start) for linear ramp
alpha_end: 0.8                          # End point (P_end) for linear ramp
td_init_steps: 50                       # Number of training steps to compute baseline TD_init

# Lamarckian Co-evolution (SGD Injection into Evolution)
use_lamarckian_sgd: True                # Enable Lamarckian SGD finetuning of elites
num_finetune_elites: 3                  # Number of top elites to finetune with SGD
lamarckian_sgd_steps: 1                 # Number of SGD steps per elite
finetune_lr: 0.0001                     # Learning rate for Lamarckian finetuning (conservative)
max_finetune_loss: 100.0                # Maximum loss threshold for finetuning (prevent divergence)
finetune_grad_clip: 5.0                 # Gradient clipping for finetuning (prevent explosion)

# RL → Evolution Injection Strategy
use_rl_injection: True                 # Enable legacy RL → Evo injection (replaced by new logic)

# --- Attacker Lamarckian SGD (Co-evolution Enhancement) ---
# Enable gradient-based fine-tuning for elite attackers to optimize quality objective
# This creates an evolutionary arms race: defenders evolve robustness → attackers break defenses
use_attacker_lamarckian_sgd: True       # Enable Lamarckian SGD fine-tuning for elite attackers
                                        # Optimizes quality (negative defender reward) objective
                                        
num_finetune_attackers: 3               # Number of top elite attackers to fine-tune
                                        # Recommended: 3-5, more increases computation cost
                                        # These are the best performers from Pareto front
                                        
attacker_lamarckian_sgd_steps: 1        # Number of SGD optimization steps per attacker
                                        # 1-3 steps recommended to avoid overfitting
                                        # More steps = stronger adaptation but potential overfitting
                                        
attacker_finetune_lr: 0.0001            # Learning rate for attacker fine-tuning
                                        # Default: attack_lr * 0.1 (conservative)
                                        # Increase if quality loss decreases slowly
                                        # Decrease if training becomes unstable
                                        
attacker_finetune_reg: 0.01             # Sparsity regularization weight (KL divergence)
                                        # Controls victim selection diversity
                                        # Higher = more sparse/stealthy attacks
                                        # Lower = more aggressive attacks
                                        
attacker_finetune_grad_clip: 5.0        # Gradient clipping threshold for stability
                                        # Default: grad_norm_clip * 0.5 (more conservative)
                                        # Decrease if gradients explode (NaN/Inf)
                                        # Prevents gradient explosion during fine-tuning


# --- Attacker parameters ---
population_train_steps: 6
individual_sample_episode: 4
spare_lambda: 0.04
attack_batch_size: 10
attacker_eval_num: 4
smoothing_factor: 0.02
# shaping_reward: True
one_buffer: True
concat_left_time: True
attack_lr: 0.0002
attacker_hidden_dim: 64
attacker_soft_tau: 0.005
attack_agent_selector: "sparse"
adversarial_training: True
num_attack_train: 12
num_attack_test: 10
sparse_ref_delta: 0.05
load_sparse_ref_delta: 0.05
attacker_pop_size: 20

name: "EvoQ_env=8_adam_td_lambda"
