# --- EvoQ specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 100000 # 500000 for 6h_vs_8z

runner: "parallel_robust"
batch_size_run: 8 # batch_size_run=4, buffer_size = 2500, batch_size=64  for 3s5z_vs_3s6z
buffer_size: 5000 
batch_size: 128
optimizer: 'adam'

t_max: 10050000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
mac: "attack_mac"
agent: "n_rnn"
agent_output_type: q

learner: "EvoQ_learner"
mixer: "qmix"
mixing_embed_dim: 32
hypernet_embed: 64
lr: 0.001 # Learning rate for agents
td_lambda: 0.6 # 0.3 for 6h_vs_8z
optimizer: 'adam'
q_lambda: False

# rnn layer normalization
use_layer_norm: False

# orthogonal init for DNN
use_orthogonal: False
gain: 0.01

# Priority experience replay
use_per: False
per_alpha: 0.6
per_beta: 0.4
return_priority: False


# --- Evolution settings ---
pop_size: 10
elite_size: 0.2

# --- Evolutionary Mechanisms ---

# Mechanism 1: Twin Adversarial Confidence (Robustness Metric)
use_twin_adversarial_confidence: True  # Use Twin-Q networks for robust confidence estimation
adversarial_tau: 1.0                    # Boltzmann temperature for policy distribution (softmax temperature)

# Mechanism 2: Evolutionary Consensus (Population-Level Robustness Calibration)
use_evolutionary_consensus: True       # Enable population consensus metric (swarm intelligence)
consensus_elite_size: 3                 # Number of elite agents for consensus calculation
consensus_fault_type: "worst"           # Fault injection type for consensus: "worst", "random", "zero"

# Mechanism 3: Adversarial Behavioral Novelty (Quality-Diversity)
use_adversarial_novelty: True          # Enable archive-based novelty search (prevent convergence)
archive_max_size: 50                    # Maximum size of elite behavior archive
novelty_k_nearest: 5                    # K for K-nearest neighbor novelty calculation
novelty_delta: 0.1                      # Perturbation magnitude for influence calculation

# Mechanism 4: Byzantine Fault Tolerance - Fault Isolation Ratio
fault_type: "worst"                     # Byzantine fault type: "worst", "random", "zero"

# Mechanism 5: Byzantine Fault Tolerance - Influence Constraint (Power Balance)
influence_delta: 0.1                    # δ for measuring agent influence: Q_k → Q_k + δ
influence_lambda: 0.5                   # λ weight for influence variance penalty

# Learning-Assisted Dynamic Weighting (Adaptive Phase-specific Multi-Objective Optimization)
use_linear_alpha: True                 # Use linear ramp (True) or sigmoid (False) for alpha_t
alpha_steepness: 10.0                   # Steepness (k) of sigmoid for alpha_t transfer function
alpha_center: 0.5                       # Center point (P_center) for sigmoid
alpha_start: 0.3                        # Start point (P_start) for linear ramp
alpha_end: 0.8                          # End point (P_end) for linear ramp
td_init_steps: 50                       # Number of training steps to compute baseline TD_init

# Lamarckian Co-evolution (SGD Injection into Evolution)
use_lamarckian_sgd: True                # Enable Lamarckian SGD finetuning of elites
num_finetune_elites: 3                  # Number of top elites to finetune with SGD
lamarckian_sgd_steps: 1                 # Number of SGD steps per elite
finetune_lr: 0.0001                     # Learning rate for Lamarckian finetuning (conservative)
max_finetune_loss: 100.0                # Maximum loss threshold for finetuning (prevent divergence)
finetune_grad_clip: 5.0                 # Gradient clipping for finetuning (prevent explosion)

# RL → Evolution Injection Strategy
use_rl_injection: True                 # Enable legacy RL → Evo injection (replaced by new logic)


# --- Attacker parameters ---
population_train_steps: 6
population_train_num: 1
individual_sample_episode: 4
save_archive_interval: 400
select_strategy: "random"
spare_lambda: 0.04
attack_batch_size: 10
min_jsdloss_sample: 64
attacker_eval_num: 4
smoothing_factor: 0.02
gen_random_start: 0.4
gen_random_end: 0.05
threshold_ratio_start: 0.9
threshold_ratio_end: 0.5
attack_nepisode: 5
default_nepisode: 5
truncation: False
shaping_reward: True
one_buffer: True
#seed: 2
concat_left_time: True
penalty: True
penalty_weight: 0.5
long_eval_interval: 100
attack_lr: 0.0002
ego_train_step: 10
go_on: False
train_attack_threshold: 0

archive_size: 15
jsd_beta: 1.2
eval_num: 1000
generation: 800
finetune_gen: 500
archive_load_path: ""
ego_agent_path: ""
start_eval: False
gpu_id: 0
attack_num: 6
test_attack_num: 6
fine_tune: False
run_type: "robust" # robust, attack, attack_na, robust_na, eval_na
test_attacker_archive_path: ""
diversity: True
sparse_ref_delta: 0.05
#sparse_ref_delta: 0
load_sparse_ref_delta: 0.05
train_random: False
attacker_hidden_dim: 64
attacker_soft_tau: 0.005
attack_agent_selector: "sparse"
adversarial_training: True
num_attack_train: 12
num_attack_test: 12
attack_mode: "action"
# --- Core Adversarial Training Switch ---
attacker_pop_size: 20               

name: "EvoQ_env=8_adam_td_lambda"
