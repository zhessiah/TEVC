# --- MARCO specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 100000 # 500000 for 6h_vs_8z

runner: "parallel_robust"
batch_size_run: 8 # batch_size_run=4, buffer_size = 2500, batch_size=64  for 3s5z_vs_3s6z
buffer_size: 5000 
batch_size: 128
optimizer: 'adam'

t_max: 10050000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
mac: "attack_mac"
agent: "n_rnn"
agent_output_type: q

learner: "MARCO_learner"
mixer: "qmix"
mixing_embed_dim: 32
hypernet_embed: 64
lr: 0.001 # Learning rate for agents
td_lambda: 0.6 # 0.3 for 6h_vs_8z
q_lambda: False

# rnn layer normalization
use_layer_norm: False

# orthogonal init for DNN
use_orthogonal: False
gain: 0.01

# Priority experience replay
use_per: False
per_alpha: 0.6
per_beta: 0.4
return_priority: False

# ============================================================================
# Evolution Settings
# ============================================================================
pop_size: 10                           # Defender population size
elite_size: 0.2                        # Elite ratio for selection (20%)

# --- Defender Fitness (2 Objectives) ---
# Objective 1 (Optimality): TD Error - minimize environment fitting error
# Objective 2 (Robustness): Fault Isolation Ratio - minimize sensitivity to faults
fault_type: "worst"                    # Byzantine fault type: "worst", "random", "zero"

# --- Learning-Assisted Dynamic Weighting ---
# Adaptive curriculum: balance optimality vs robustness based on learning progress
use_linear_alpha: True                 # Use linear ramp (True) or sigmoid (False)
alpha_steepness: 10.0                  # Steepness for sigmoid transfer function
alpha_center: 0.5                      # Center point for sigmoid
alpha_start: 0.3                       # Start point for linear ramp
alpha_end: 0.8                         # End point for linear ramp
td_init_steps: 50                      # Steps to compute baseline TD_init

# ============================================================================
# Attacker Evolution Settings
# ============================================================================
attacker_pop_size: 20                  # Attacker population size

# --- Attacker Fitness (2 Objectives) ---
# Objective 1 (Quality): Negative defender reward - maximize damage
# Objective 2 (Novelty): Behavioral diversity - explore attack space

# --- Attack Budget & Advantage Function ---
# New mechanism (V6): Budget-Modulated Attack Advantage
# A_att(j|s,k) = Q_j(s,u_j) - γ(k)·Q̄(s)
# γ(k) = 1 + λ(K-k)/K
budget_lambda: 1.0                     # Budget modulation strength λ
                                       # Higher = more conservative when budget low
                                       # Recommended: 0.5-2.0

# --- Attacker Memetic SGD (Policy Gradient with Attack Advantage) ---
# REFACTORED: Uses attack advantage + policy gradient loss
# L_att = -E[Σ A_att(j)·log π(j)]
attacker_finetune_lr: 0.00002          # Learning rate for memetic SGD (attack_lr * 0.1)
                                       # Conservative to avoid breaking evolved structure

# --- Attacker Parameters ---
population_train_steps: 6              # Training steps per evolution generation
individual_sample_episode: 4           # Episodes sampled per individual evaluation
spare_lambda: 0.04                     # Sparsity parameter
attack_batch_size: 10                  # Batch size for attacker training
attacker_eval_num: 4                   # Number of episodes for attacker evaluation
smoothing_factor: 0.02                 # Smoothing factor for attacker metrics

one_buffer: True                       # Use single replay buffer for all attackers
concat_left_time: True                 # Concatenate remaining budget to attacker state
attack_lr: 0.002                      # Base learning rate for attackers
attacker_hidden_dim: 64                # Hidden dimension for attacker network
attacker_soft_tau: 0.005               # Soft update parameter for attacker target network
attack_agent_selector: "sparse"        # Action selector for attackers

adversarial_training: True             # Enable adversarial training
num_attack_train: 30                   # Number of training attacks per episode
num_attack_test: 30                   # Number of test attacks per episode
sparse_ref_delta: 0.05                 # Delta for sparse reference distribution
load_sparse_ref_delta: 0.05            # Delta for loading sparse reference

name: "MARCO_env=8_adam_td_lambda"
